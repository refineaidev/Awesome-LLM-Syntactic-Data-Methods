# Syntactic Data Methods

| Name | Year | Venue | Paper Link | Blog Link |
|------|------|-------|------------|-----------|
| **Mind the Gap: Diverse NMT Models for Resource-Constrained Environments** | 2025 | ACL | [paper](https://aclanthology.org/2025.nodalida-1.21.pdf) | [Blog](https://www.refineai.dev/en/blog/mind_the_gap) |
| **Automatic Instruction Evolving for Large Language Models** | 2024 | arXiv | [paper](https://arxiv.org/abs/2406.00770) | [Blog](https://www.refineai.dev/en/blog/auto) |
| **Scaling Synthetic Data Creation with 1,000,000,000 Personas** | 2024 | arXiv | [paper](https://arxiv.org/abs/2406.20094) | [Blog](https://www.refineai.dev/en/blog/1b_personas) |
| **Self-Play Fine-Tuning Converts Weak Language Models to Strong Language Models** | 2024 | ICML | [paper](https://arxiv.org/abs/2401.01335) | — |
| **Self-Rewarding Language Models** | 2024 | arXiv | [paper](https://arxiv.org/abs/2401.10020) | — |
| **Synthetic Data (Almost) from Scratch: Generalized Instruction Tuning for Language Models** | 2024 | arXiv | [paper](https://arxiv.org/abs/2402.13064) | — |
| **Rephrasing the Web: A Recipe for Compute and Data-Efficient Language Modeling** | 2024 | ACL | [paper](https://arxiv.org/abs/2401.16380) | — |
| **TarGEN: Targeted Data Generation with Large Language Models** | 2024 | COLM | [paper](https://arxiv.org/abs/2310.17876) | — |
| **Instruction Pre-Training: Language Models are Supervised Multitask Learners** | 2024 | arXiv | [paper](https://arxiv.org/pdf/2406.14491) | — |
| **Self-playing Adversarial Language Game Enhances LLM Reasoning** | 2024 | arXiv | [paper](https://arxiv.org/abs/2404.10642) | — |
| **Source2Synth: Synthetic Data Generation and Curation Grounded in Real Data Sources** | 2024 | arXiv | [paper](https://arxiv.org/abs/2409.08239) | — |
| **Learning to Generate Instruction Tuning Datasets for Zero-Shot Task Adaptation** | 2024 | ACL | [paper](https://arxiv.org/abs/2402.18334) | — |
| **Magpie: Alignment Data Synthesis from Scratch by Prompting Aligned LLMs with Nothing** | 2024 | arXiv | [paper](https://arxiv.org/abs/2406.08464) | — |
| **On the Diversity of Synthetic Data and its Impact on Training Large Language Models** | 2024 | arXiv | [paper](https://arxiv.org/abs/2410.15226) | — |
| **CodecLM: Aligning Language Models with Tailored Synthetic Data** | 2024 | NAACL | [paper](https://arxiv.org/abs/2404.05875) | — |
| **Large Language Models Can Self-Improve** | 2023 | EMNLP | [paper](https://aclanthology.org/2023.emnlp-main.67/) | — |
| **CAMEL: Communicative Agents for “Mind” Exploration of Large Language Model Society** | 2023 | NeurIPS | [paper](https://arxiv.org/abs/2303.17760) | — |
| **Self-instruct: Aligning language models with self-generated instructions** | 2023 | ACL | [paper](https://arxiv.org/abs/2212.10560) | — |
| **WizardLM: Empowering Large Language Models to Follow Complex Instructions** | 2023 | arXiv | [paper](https://arxiv.org/abs/2304.12244) | — |
| **STaR: Bootstrapping Reasoning With Reasoning** | 2022 | NeurIPS | [paper](https://arxiv.org/abs/2203.14465) | — |
| **Symbolic Knowledge Distillation: from General Language Models to Commonsense Models** | 2022 | NAACL | [paper](https://aclanthology.org/2022.naacl-main.341.pdf) | — |
| **Generating Training Data with Language Models: Towards Zero-Shot Language Understanding** | 2022 | NeurIPS | [paper](https://arxiv.org/abs/2202.04538) | — |
| **ZeroGen: Efficient Zero-shot Learning via Dataset Generation** | 2022 | EMNLP | [paper](https://arxiv.org/abs/2202.07922) | — |

